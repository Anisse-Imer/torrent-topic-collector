services:
  spark-master:
    container_name: da-spark-master
    build: .
    image: da-spark-image
    entrypoint: ['/opt/spark/apps/entrypoint.sh', 'master']
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ./spark_apps:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events
    env_file:
      - ./spark_apps/.env.spark
    ports:
      - '9090:8080'
      - '7077:7077'
    networks:
      - data_lakehouse_net
  
  spark-history-server:
    container_name: da-spark-history
    image: da-spark-image
    entrypoint: ['/opt/spark/apps/entrypoint.sh', 'history']
    depends_on:
      - spark-master
    env_file:
      - ./spark_apps/.env.spark
    volumes:
      - ./spark_apps:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'
    networks:
      - data_lakehouse_net
  
  spark-worker:
    # container_name: spark-worker
    image: da-spark-image
    entrypoint: ['/opt/spark/apps/entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    env_file:
      - ./spark_apps/.env.spark
    volumes:
      - ./spark_apps:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events
    networks:
      - data_lakehouse_net
  
  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - ALLOW_PLAINTEXT_LISTENER=yes
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - data_lakehouse_net
  
  hdfs-namenode:
    image: apache/hadoop:3.4.1
    container_name: hdfs-namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_cluster/namenode:/opt/hadoop/data/nameNode
      - ./hadoop_cluster/config:/opt/hadoop/etc/hadoop
      - ./hadoop_cluster/start-hdfs.sh:/start-hdfs.sh
      - ./hadoop_cluster/config/log4j.properties:/opt/hadoop/etc/hadoop/log4j.properties
    ports:
      - "9870:9870"
    command: [ "/bin/bash", "/start-hdfs.sh" ]
    restart: always
    networks:
      data_lakehouse_net:

  hdfs-datanode1:
    image: apache/hadoop:3.4.1
    container_name: hdfs-datanode1
    hostname: datanode1
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_cluster/datanode1:/opt/hadoop/data/dataNode
      - ./hadoop_cluster/config:/opt/hadoop/etc/hadoop
      - ./hadoop_cluster/init-datanode.sh:/init-datanode.sh
      - ./hadoop_cluster/config/log4j.properties:/opt/hadoop/etc/hadoop/log4j.properties
    depends_on:
      - hdfs-namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    restart: always
    networks:
      data_lakehouse_net:
  
  hdfs-datanode2:
    image: apache/hadoop:3.4.1
    container_name: hdfs-datanode2
    hostname: datanode2
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_cluster/datanode2:/opt/hadoop/data/dataNode
      - ./hadoop_cluster/config:/opt/hadoop/etc/hadoop
      - ./hadoop_cluster/init-datanode.sh:/init-datanode.sh
      - ./hadoop_cluster/config/log4j.properties:/opt/hadoop/etc/hadoop/log4j.properties
    depends_on:
      - hdfs-namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    restart: always
    networks:
      data_lakehouse_net:

networks:
  data_lakehouse_net:
    name: data_lakehouse_net
    driver: bridge

volumes:
  spark-logs:
  kafka_data:
